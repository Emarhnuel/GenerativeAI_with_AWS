{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we'll explore how to host a large language model on Amazon SageMaker using Hugging Face LLM Inference Container for Amazon SageMaker, which allows you to easily deploy the most popular open-source LLMs, including\n",
        "Phi-3, Falcon, StarCoder, BLOOM, GPT-NeoX, Llama, and T5.\n",
        "\n",
        "We are going to use the SageMaker Python SDK to deploy Phi-3-medium-128k-instruct model to Amazon SageMaker."
      ],
      "metadata": {
        "id": "HfY78N4mx6Sn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbh0kR3-xRZ6"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade boto3 sagemaker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we begin with the actual work for packaging and deploying the model to Amazon SageMaker, we need to setup the notebook environment respectively. This includes:\n",
        "\n",
        "1. retrieval of the execution role our SageMaker Studio domain is associated with for later usage\n",
        "\n",
        "2. retrieval of our bucket for later usage\n",
        "3. retrieval of the chosen region for later usage"
      ],
      "metadata": {
        "id": "yjQS6oCkyhBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "sess = sagemaker.Session()\n",
        "# sagemaker session bucket -> used for uploading data, models and logs\n",
        "# sagemaker will automatically create this bucket if it not exists\n",
        "sagemaker_session_bucket=None\n",
        "if sagemaker_session_bucket is None and sess is not None:\n",
        "    # set to default bucket if a bucket name is not given\n",
        "    sagemaker_session_bucket = sess.default_bucket()\n",
        "\n",
        "try:\n",
        "    role = sagemaker.get_execution_role()\n",
        "except ValueError:\n",
        "    iam = boto3.client('iam')\n",
        "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
        "\n",
        "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
        "\n",
        "print(f\"sagemaker role arn: {role}\")\n",
        "print(f\"sagemaker session region: {sess.boto_region_name}\")"
      ],
      "metadata": {
        "id": "pm0BFGwrywna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to deploying regular Hugging Face models, we first need to retrieve the container uri and provide it to our HuggingFaceModel model class with a image_uri pointing to the image.\n",
        "\n",
        "To retrieve the new Hugging Face LLM Deep Learning Container in Amazon SageMaker, we can use the get_huggingface_llm_image_uri method provided by the SageMaker SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified backend, session, region, and version."
      ],
      "metadata": {
        "id": "7Vp39m3Dy9ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
        "\n",
        "# retrieve the llm image uri\n",
        "llm_image = get_huggingface_llm_image_uri(\n",
        "  \"huggingface\",\n",
        "  version=\"0.8.2\"\n",
        ")\n",
        "\n",
        "# print ecr image uri\n",
        "print(f\"llm image uri: {llm_image}\")"
      ],
      "metadata": {
        "id": "UAhmEda6y4Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deploy Phi-3-medium model to Amazon SageMaker, we create a HuggingFaceModel model class and define our endpoint configuration including the hf_model_id, and instance_type. We will use a g5.12xlarge instance type with 4 NVIDIA A10G GPUs and 96GB of GPU memory."
      ],
      "metadata": {
        "id": "YO98Z-eLzG2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "# sagemaker config\n",
        "instance_type = \"ml.g5.12xlarge\"\n",
        "number_of_gpu = 4\n",
        "\n",
        "# TGI config\n",
        "config = {\n",
        "  'HF_MODEL_ID': \"microsoft/Phi-3-medium-128k-instruct\", # model id from hf.co/models\n",
        "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
        "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
        "  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
        "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
        "}\n",
        "\n",
        "# create HuggingFaceModel\n",
        "llm_model = HuggingFaceModel(\n",
        "  role=role,\n",
        "  image_uri=llm_image,\n",
        "  env=config\n",
        ")\n"
      ],
      "metadata": {
        "id": "lPTjKBqKzMY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we have created the HuggingFaceModel we can deploy it to Amazon SageMaker using the deploy method. We will deploy the model with the ml.g5.12xlarge instance type. The Hugging Face LLM Deep Learning Container is powered by Text Generation Inference (TGI), an open-source, purpose-built solution for deploying and serving Large Language Models.TGI will automatically distribute and shard the model across all GPUs."
      ],
      "metadata": {
        "id": "dPMI1zonzWly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy model to an endpoint\n",
        "llm = llm_model.deploy(\n",
        "  initial_instance_count=1,\n",
        "  instance_type=instance_type,\n",
        "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
        ")"
      ],
      "metadata": {
        "id": "3wGt44pXzTlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After our endpoint is deployed we can run inference on it using the predict method from the predictor. We can use different parameters to control the generation, defining them in the parameters attribute of the payload. As of today TGI supports the following parameters:\n",
        "\n",
        "temperature: Controls randomness in the model. Lower values will make the model more deterministic and higher values will make the model more random. Default value is 1.0.\n",
        "\n",
        "max_new_tokens: The maximum number of tokens to generate. Default value is 20, max value is 512.\n",
        "\n",
        "repetition_penalty: Controls the likelihood of repetition, defaults to null.\n",
        "seed: The seed to use for random generation, default is null.\n",
        "\n",
        "stop: A list of tokens to stop the generation. The generation will stop when one of the tokens is generated.\n",
        "\n",
        "top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Default value is null, which disables top-k-filtering.\n",
        "\n",
        "top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling, default to null\n",
        "\n",
        "do_sample: Whether or not to use sampling; use greedy decoding otherwise. Default value is false.\n",
        "\n",
        "best_of: Generate best_of sequences and return the one if the highest token logprobs, default to null.\n",
        "\n",
        "details: Whether or not to return details about the generation. Default value is false.\n",
        "\n",
        "return_full_text: Whether or not to return the full text or only the generated part. Default value is false.\n",
        "\n",
        "truncate: Whether or not to truncate the input to the maximum length of the model. Default value is true.\n",
        "\n",
        "typical_p: The typical probability of a token. Default value is null.\n",
        "\n",
        "watermark: The watermark to use for the generation. Default value is false."
      ],
      "metadata": {
        "id": "4OFBt29uzpCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define payload\n",
        "prompt = \"\"\"You are an helpful Assistant, called Falcon. Knowing everyting about AWS.\n",
        "\n",
        "User: Can you tell me something about Amazon SageMaker?\n",
        "Falcon:\"\"\"\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "P9mZDfDzzzKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering\n",
        "Prompt engineering is a technique used to design effective prompts for LLMs with the goal to achieve:\n",
        "\n",
        "1. Control over the output: With prompt engineering, developers can control the output generated by LLMs. By designing prompts that specify the desired topic, style, tone, and level of formality, they can guide the LLM to produce text that meets the desired criteria.\n",
        "2. Mitigating bias: LLMs have been shown to produce biased outputs when prompted with certain topics or language patterns. By engineering prompts that avoid biased language and encourage fairness, developers can help mitigate these issues.\n",
        "3. Improving efficiency: Prompt engineering can help LLMs work more efficiently by guiding them to generate the desired output with fewer iterations. By providing clear, concise, and specific prompts, developers can help LLMs achieve the desired outcome faster and with fewer errors.\n",
        "\n",
        "In general, a prompt can contain any of the following components:\n",
        "\n",
        "1. Instruction - a specific task or instruction you want the model to perform\n",
        "2. Context - can involve external information or additional context that can steer the model to better responses\n",
        "3. Input Data - is the input or question that we are interested to find a response for\n",
        "4. Output Indicator - indicates the type or format of output.\n",
        "\n",
        "In general, the more information we provide with the prompt the better the above mentioned goals will be achieved.\n"
      ],
      "metadata": {
        "id": "5npSZEliz_lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple unstructured prompt\n",
        "prompt = \"\"\"\n",
        "Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "User: What was OKT3 originally sourced from?\n",
        "\n",
        "Falcon:\"\"\"\n",
        "\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "SyOqm1LV0YzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We now stick to the scheme proposed above\n",
        "prompt = \"\"\"\n",
        "Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "BtHcvIy50dmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, few-shot learning is an interesting approach for the context element of a prompt. Few-shot learning is a prompt engineering technique that enables models to learn new tasks or concepts from only a few examples (usually a single digit number is just fine) or samples. Despite of the fact that the model has never seen this task in the training phase, we experience a significant boost in performance."
      ],
      "metadata": {
        "id": "dmFdg0Hm0lXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-shot\n",
        "prompt = \"\"\"\n",
        "Tweet: \"This new music video was incredibile\"\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "uhceKyba0hFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot\n",
        "prompt = \"\"\"\n",
        "Tweet: \"I hate it when my phone battery dies.\"\n",
        "Sentiment: Negative\n",
        "###\n",
        "Tweet: \"My day has been üëç\"\n",
        "Sentiment: Positive\n",
        "###\n",
        "Tweet: \"This is the link to the article\"\n",
        "Sentiment: Neutral\n",
        "###\n",
        "Tweet: \"This new music video was incredibile\"\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9,\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "for seq in response:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "nmuh_b8F1Gh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.delete_model()\n",
        "llm.delete_endpoint()"
      ],
      "metadata": {
        "id": "ZHBHJETC1R3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}